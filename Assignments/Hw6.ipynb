{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5430_Assignment6_chb2132.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "XF71hamoNiJh"
      },
      "source": [
        "# jupyter-lab --NotebookApp.iopub_data_rate_limit=1e10\n",
        "# --NotebookApp.iopub_data_rate_limit = 1e10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wO66EEqjH7jI"
      },
      "source": [
        "## **Overview**\n",
        "---\n",
        ">**A. Download GoogleNews-vectors-negative300.bin.gz pre-trained model**\n",
        ">\n",
        ">**B. Write a Python program, which:**\n",
        ">>\n",
        ">>**1.** Loads the downloaded pre-trained Google Word2Vec model\n",
        ">>\n",
        ">>**2.** Loads your previously obtained dataset of Webhose news articles\n",
        ">>\n",
        ">>**3.** For a selected article title from the dataset:\n",
        ">>>  * Finds 100 most similar titles based on Word2Vec similarity, and\n",
        ">>>  * Prints those titles in a descending order of similarity scores\n",
        ">\n",
        ">**C. Write a Pyspark program, which:**\n",
        ">>\n",
        ">>**1.** Loads your previously obtained dataset of Webhose news articles into a Spark dataframe\n",
        ">>\n",
        ">>**2.** Cleans + tokenizes article bodies using RegexTokenizer + Stopword remover functions\n",
        ">>\n",
        ">>**3.** Trains a Word2Vec model based on the output column produced in prior step\n",
        ">>\n",
        ">>**4.** Implements any sample search query and produces matching article titles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnHcrGYSKeyA"
      },
      "source": [
        "##**B. Write a Python Program, which:**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzXjICWiJjdI"
      },
      "source": [
        "###*1. Loads the downloaded pre-trained Google Word2Vec model*\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bQCBILkKeUU"
      },
      "source": [
        "import json\n",
        "import spacy\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import en_core_web_sm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emG0eu33W-DX"
      },
      "source": [
        "import gensim, operator\n",
        "from gensim.models import KeyedVectors, Word2Vec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53AS7Rpjvz9L"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFJdyG2dtGij"
      },
      "source": [
        "# loads in a model from specified path & file\n",
        "model_path = '/content/drive/MyDrive/'\n",
        "\n",
        "def load_model(model_name, file_name, flagBin):\n",
        "\n",
        "    print('Loading ' + model_name + ' model...')\n",
        "    model = KeyedVectors.load_word2vec_format(model_path + file_name, binary = flagBin)\n",
        "    print('Finished loading ' + model_name + ' model...')\n",
        "    \n",
        "    return model\n",
        "   \n",
        "# load in Google word2vec model\n",
        "model_w2v = load_model('Word2Vec', 'GoogleNews-vectors-negative300.bin.gz', True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3J6cdw7Jq5s"
      },
      "source": [
        "### **2. Loads your previously obtained dataset of Webhose news articles**\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLfGmq0g1qqL"
      },
      "source": [
        "# webhose_json = pd.read_json('/content/drive/MyDrive/webhose_apple.json', lines = True)\n",
        "# webhose_csv = webhose_json.to_csv('/content/webhose_apple.csv')\n",
        "\n",
        "#newsfeeds = pd.read_csv('webhose_apple.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOuE08mnSQ2O"
      },
      "source": [
        "apple_data = []\n",
        "\n",
        "with open('/content/drive/MyDrive/webhose_apple.json', 'r') as f:\n",
        "    for line in f.readlines():\n",
        "        apple_data.append(json.loads(line))\n",
        "\n",
        "apple_titles = [a['title'] for a in apple_data]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZXQbrclfZvO"
      },
      "source": [
        "# subsetting feed titles\n",
        "feeds = []\n",
        "feeds.append(apple_titles[:2132])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqzLIGwJJuLE"
      },
      "source": [
        "### **3. For a selected article title from the dataset:**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OUeL1uVXKWI"
      },
      "source": [
        "#### *Define Vector Model Functions*\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iO3CDse6XMll"
      },
      "source": [
        "# check if input words in model vocabulary\n",
        "def check_vocab(model, words):\n",
        "    \n",
        "    check_words = list()\n",
        "\n",
        "    for word in words:\n",
        "        if word in model.vocab:\n",
        "            check_words.append(word.strip())\n",
        "            \n",
        "    return check_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RF1m1rhXXOxN"
      },
      "source": [
        "# calculate string similarity with model\n",
        "def calc_sim(s1, s2, model):\n",
        "\n",
        "    s1_terms = set(check_vocab(model, s1.split()))\n",
        "    s2_terms = set(check_vocab(model, s2.split()))\n",
        "    \n",
        "    str_sim = model.similarity(s1_terms, s2_terms)\n",
        "\n",
        "    return str_sim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJbQV_4nO6cc"
      },
      "source": [
        "# calculate vector similarity with particular model\n",
        "def vec_sim(v1, v2, model):\n",
        "\n",
        "    vec_terms = [np.zeros(300), np.zeros(300)]\n",
        "    vectors = [v1, v2]\n",
        "        \n",
        "    for term, vec in enumerate(vectors):\n",
        "        for t, v in enumerate(vec.split(' ')):\n",
        "            try:\n",
        "                vec_terms[term] += model[v]\n",
        "            except:\n",
        "                vec_terms[term] += 0\n",
        "        \n",
        "    vec_sim = (1 - spatial.distance.cosine(vec_terms[0], vec_terms[1]))\n",
        "\n",
        "    if vec_sim is 'nan':\n",
        "        vec_sim = 0\n",
        "        \n",
        "    return vec_sim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNfFQsN_Uxx_"
      },
      "source": [
        "# find the cosine similarity of two vectors\n",
        "def cos_sim(v1, v2):\n",
        "\n",
        "    return np.dot(v1, v2) / np.sqrt(np.dot(v1, v1)) / np.sqrt(np.dot(v2, v2)+.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z82Ry6I0PeI4"
      },
      "source": [
        "#### **a. Find 100 most similar titles based on Word2Vec similarity**\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uw6QI3wiinY6"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "  \n",
        "  dex = 888\n",
        "  apple_calc = []\n",
        "\n",
        "  for title in apple_titles:\n",
        "\n",
        "    s1 = str(apple_titles[dex])\n",
        "    s2 = str(title)\n",
        "    \n",
        "    apple_sim = calc_sim(s1, s2, model_w2v)  \n",
        "    apple_calc.append(apple_sim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DelREnNQPtwB"
      },
      "source": [
        "#### **b. Prints those titles in a descending order of similarity scores**\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCidjqwkxxr9"
      },
      "source": [
        "apple_dict = {'title':apple_titles, 'similarity':apple_calc}\n",
        "\n",
        "apple_df = pd.DataFrame(apple_dict).\\\n",
        "  orderBy('similarity', ascending = False)\n",
        "\n",
        "apple_df.show(100, truncate = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkMYskKUKjAU"
      },
      "source": [
        "#**C. Write a Pyspark program, which:**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPSj5Ay4Kokk"
      },
      "source": [
        "###**1. Loads your previously obtained dataset of Webhose news articles into a Spark dataframe**\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CB2MjfDmPRlL"
      },
      "source": [
        "!pip install pyspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-_xfAwoB1Af"
      },
      "source": [
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SQLContext\n",
        "\n",
        "sc = SparkContext() \n",
        "sqlContext = SQLContext(sc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IR6IjdwcrCDn"
      },
      "source": [
        "from scipy import spatial\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "\n",
        "from pyspark.mllib.linalg import Vector, Vectors\n",
        "from pyspark.mllib.clustering import LDA, LDAModel\n",
        "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, Word2Vec, Word2VecModel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlyZSm1-Y03k"
      },
      "source": [
        "article_df = sqlContext.read.option(\"header\", \"true\").option(\"delimiter\", \",\") \\\n",
        "                    .option(\"inferSchema\", \"true\") \\\n",
        "                    .json(\"/content/drive/MyDrive/webhose_apple.json\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_1ExgzjY7PY"
      },
      "source": [
        "article_data = article_df['index', 'title']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFcqcmeEwGBj"
      },
      "source": [
        "article_columns = [0,1]\n",
        "\n",
        "article_rdd = article_data.select('*') \\\n",
        "                       .rdd.map(lambda row: [row[i] for i in article_columns]) \\\n",
        "                       .filter(lambda row: row[1] is not None)\n",
        "\n",
        "article_df = sqlContext.createDataFrame(article_rdd, ['index', 'title'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xU7I34_Ks3k"
      },
      "source": [
        "##**2. Cleans + tokenizes article bodies using RegexTokenizer + Stopword remover functions**\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eU4hll4Ix4p8"
      },
      "source": [
        "nlp = en_core_web_sm.load( disable=['parser', 'tagger','ner'] )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QXkG-s3x7DP"
      },
      "source": [
        "def cleanup_pretokenize(text):\n",
        "    #text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "    text = text.replace(\"'s\", \" \")\n",
        "    text = text.replace(\"n't\", \" not \")\n",
        "    text = text.replace(\"'ve\", \" have \")\n",
        "    text = text.replace(\"'re\", \" are \")\n",
        "    text = text.replace(\"I'm\",\" I am \")\n",
        "    text = text.replace(\"you're\",\" you are \")\n",
        "    text = text.replace(\"You're\",\" You are \")\n",
        "    text = text.replace(\"-\",\" \")\n",
        "    text = text.replace(\"/\",\" \")\n",
        "    text = text.replace(\"(\",\" \")\n",
        "    text = text.replace(\")\",\" \")\n",
        "    text = text.replace(\"%\",\" percent \")\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHvDJmnGyBkf"
      },
      "source": [
        "lmtzr = WordNetLemmatizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRFQYwseYmfQ"
      },
      "source": [
        "def text_cleanup(row):\n",
        "    desc = row[2].strip().lower()\n",
        "    tokens = [w.lemma_ for w in nlp(cleanup_pretokenize(desc))]\n",
        "    tokens = [token for token in tokens if token.isalpha()]\n",
        "    tokens = [token for token in tokens if len(token) > 3]\n",
        "    #tokens = [lmtzr.lemmatize(token,'v') for token in tokens]\n",
        "    row[2] = ' '.join(tokens)\n",
        "    return row"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFCFGFNfYjGE"
      },
      "source": [
        "regexTokenizer = RegexTokenizer(gaps = False, pattern = '\\w+', inputCol = 'description', outputCol = 'tokens')\n",
        "swr = StopWordsRemover(inputCol = 'tokens', outputCol = 'tokens_sw_removed')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-R1LDaczAn6"
      },
      "source": [
        "df_tokens = regexTokenizer.transform(article_df)\n",
        "desc_swr = swr.transform(df_tokens)\n",
        "desc_swr.show(3)\n",
        "\n",
        "#desc_swr_half = desc_swr.limit(50000)\n",
        "#desc_swr_half.show(3)\n",
        "#desc_swr.write.saveAsTable('desc_swr', mode = 'overwrite')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NydFlJsbK0N4"
      },
      "source": [
        "##**3. Trains a Word2Vec model based on the output column produced in prior step**\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJqjlHk_zDkG"
      },
      "source": [
        "word2vec = Word2Vec(vectorSize = 300, minCount = 5, inputCol = 'tokens_sw_removed', outputCol = 'wordvectors')\n",
        "model = word2vec.fit(desc_swr)\n",
        "wordvectors = model.transform(desc_swr)\n",
        "#wordvectors.select('wordvectors').show(1, truncate = True)\n",
        "article_desc = wordvectors.select('index','title','wordvectors').rdd.toDF()\n",
        "article_desc.show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjbXPz4eUH4c"
      },
      "source": [
        "#chunk = article_desc.filter(lambda r: r[1]>=0 and r[1]<1000).collect()\n",
        "#chunk = article_desc.collect()\n",
        "\n",
        "chunk = article_desc.take(50000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssqm-qIozGg7"
      },
      "source": [
        "synonyms = model.findSynonyms(\"apple\", 20)   \n",
        "synonyms.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tNPrfIWKz-Q"
      },
      "source": [
        "##**4. Implements any sample search query and produces matching article titles**\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubulZ8clK34Q"
      },
      "source": [
        "SEARCH_QUERY = \"Apple Music\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6kj7bEZv5V8"
      },
      "source": [
        "query_df  = sc.parallelize([(1,SEARCH_QUERY)]).toDF(['index','description'])\n",
        "query_tok = regexTokenizer.transform(query_df)\n",
        "query_swr = swr.transform(query_tok)\n",
        "query_swr.show()\n",
        "\n",
        "query_vec = model.transform(query_swr)\n",
        "query_vec = query_vec.select('wordvectors').collect()[0][0]\n",
        "query_vec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUSykMBQv6_S"
      },
      "source": [
        "sim_rdd = sc.parallelize((i[0], i[1], float(cossim(query_vec, i[2]))) for i in chunk)\n",
        "sim_df  = sqlContext.createDataFrame(sim_rdd).\\\n",
        "                   withColumnRenamed('_1', 'index').\\\n",
        "                   withColumnRenamed('_2', 'title').\\\n",
        "                   withColumnRenamed('_3', 'similarity').\\\n",
        "                   orderBy(\"similarity\", ascending = False)\n",
        "                   \n",
        "sim_df.show(42, truncate = False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
