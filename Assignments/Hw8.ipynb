{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5430_Assignment8_chb2132.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LNEm9_d-ivz"
      },
      "source": [
        "#`5430 NLP | SPRING 2021 | ASSIGNMENT 8 | UNI: CHB2132 `#\n",
        "---\n",
        "\n",
        "**A. Write a Python program based on the Week 8 class exercises, which:**\n",
        "\n",
        "* **Implement:** LDA training and topic modeling on dataset of deduplicated Webhose feeds\n",
        "  * *Modify min_df, max_df, max_features and max_iter (sklearn) to achieve the best results*\n",
        "\n",
        "* **Submit:** Code + Output a set of n topic clusters with up to 10 keywords per cluster\n",
        "   * *Clusters should not overlap, and keywords be should allowed to approximate the meaning*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPXTEmVGQLYi"
      },
      "source": [
        "#### *Library imports and utility text cleanup function*\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "koZkYwVn-gyq"
      },
      "source": [
        "!pip install sklearn\n",
        "!pip install pyLDAvis\n",
        "!pip install ipython==7.10.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TGh5RrIn5U5"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nqcR4AVPygu"
      },
      "source": [
        "import matplotlib\n",
        "import pandas as pd\n",
        "import json, re, requests"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uycEQ5LQPgYV"
      },
      "source": [
        "import pyLDAvis\n",
        "import pyLDAvis.sklearn\n",
        "pyLDAvis.enable_notebook()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgolWdo8P0HR"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUiXoWExD59w"
      },
      "source": [
        "import nltk\n",
        "import string\n",
        "\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.stem.snowball import SnowballStemmer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZEJS54NP2DO"
      },
      "source": [
        "stopwords = set(stopwords.words('english'))\n",
        "punctuation = set(string.punctuation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9nthq_aWFqY"
      },
      "source": [
        "#### Read dataset of feeds into an array and grab all titles\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7teCW6PWu75"
      },
      "source": [
        "apple_feeds = []\n",
        "\n",
        "with open('/content/dedupes.json', 'r') as f:\n",
        "  for line in f.readlines():\n",
        "    apple_feeds.append(json.loads(line))\n",
        "\n",
        "feed_titles = [feed['title'] for feed in apple_feeds]\n",
        "print(\"Total number of titles: \" + str(len(feed_titles)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2EytLpK6Q6Ca"
      },
      "source": [
        "# tokenize titles\n",
        "def tokenize_titles(title):\n",
        "    tokens = nltk.word_tokenize(title)\n",
        "    lmtzr = WordNetLemmatizer()\n",
        "    filtered_tokens = []\n",
        "    \n",
        "    for token in tokens:\n",
        "        token = token.replace(\"'s\", \" \").replace(\"n’t\", \" not\").replace(\"’ve\", \" have\")\n",
        "        token = re.sub(r'[^a-zA-Z0-9 ]', '', token)\n",
        "        if token not in stopwords:\n",
        "            filtered_tokens.append(token.lower())\n",
        "    \n",
        "    lemmas = [lmtzr.lemmatize(t,'v') for t in filtered_tokens]\n",
        "\n",
        "    return lemmas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yla87AgLRKNk"
      },
      "source": [
        "# term-document matrix\n",
        "tf_vectorizer = CountVectorizer(strip_accents = 'unicode',\n",
        "                                stop_words = 'english',\n",
        "                                lowercase = True,\n",
        "                                #tokenizer=tokenize_titles,\n",
        "                                max_features=500,\n",
        "                                token_pattern = r'\\b[a-zA-Z]{3,}\\b',\n",
        "                                max_df = 888, \n",
        "                                min_df = 2,\n",
        "                                ngram_range=(1,8))\n",
        "\n",
        "dtm_tf = tf_vectorizer.fit_transform(feed_titles)\n",
        "\n",
        "print(dtm_tf.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKyHVXTxul8O"
      },
      "source": [
        "#### *Cluster with LDA*\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4t-N2YmcRS3J"
      },
      "source": [
        "# clustering w/ LDA with online/real-time learning method\n",
        "lda_tf = LatentDirichletAllocation(n_components=10, \n",
        "                                   max_iter=200,\n",
        "                                   learning_method='online', \n",
        "                                   random_state = 0)\n",
        "\n",
        "lda_tf.fit(dtm_tf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T79JK7fKRYD1"
      },
      "source": [
        "topics = dict()\n",
        "n_top_words = 10\n",
        "\n",
        "tf_feature_names = tf_vectorizer.get_feature_names()\n",
        "\n",
        "for topic_idx, topic in enumerate(lda_tf.components_):\n",
        "    topics[topic_idx] = [tf_feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
        "    print(\"Topic #%d:\" % topic_idx)\n",
        "    print(\" | \".join([tf_feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6cC7k6_RZru"
      },
      "source": [
        "pyLDAvis.sklearn.prepare(lda_tf, dtm_tf, tf_vectorizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXZ7Mz3Z6uwW"
      },
      "source": [
        "#### *Find Dominant Topic for All (Or 10 random) Titles*\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qH_4qhxNmHP6"
      },
      "source": [
        "# Create a document to topic matrix\n",
        "lda_output = lda_tf.transform(dtm_tf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1FeDj53mxHM"
      },
      "source": [
        "# column names\n",
        "topicnames = ['Topic_' + str(i) for i in range(lda_tf.n_components)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HshnwX8mO8F"
      },
      "source": [
        "# index names\n",
        "docnames = ['Doc_' + str(i) for i in range(len(feed_titles))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZmMYvOLoeWJf"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# create dataframe with topicval and dominant topic for each title\n",
        "feed_topic = pd.DataFrame(np.round(lda_output,2), columns=topicnames, index=docnames)\n",
        "feed_topic['dominant_topic'] = np.argmax(feed_topic.values, axis=1)\n",
        "\n",
        "#feed_topic.head()\n",
        "feed_topic['dominant_topic']"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
