{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5430_Assignment9_chb2132.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "id": "UT8JmujrBz5M",
        "outputId": "432cd621-1117-4b96-b8e7-ed3d86029e13"
      },
      "source": [
        "%%html\n",
        "<marquee style='width: 30%; color: blue;'><b>5430 NLP | SPRING 2021 | ASSIGNMENT 9 | UNI: CHB2132</b></marquee>"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<marquee style='width: 30%; color: blue;'><b>5430 NLP | SPRING 2021 | ASSIGNMENT 9 | UNI: CHB2132</b></marquee>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mq7cb7fVARt0"
      },
      "source": [
        "#`5430 NLP | SPRING 2021 | ASSIGNMENT 9 | UNI: CHB2132 `#\n",
        "\n",
        "---\n",
        "\n",
        "###**Write a Python program, which:**\n",
        "\n",
        "**1. Implements a keyword or sentence taxonomy of topic classes.**\n",
        "\n",
        "**Notes:**\n",
        "* Derive taxonomy from: LDA topic clustering results and some amount of manual enrichment\n",
        "* Taxonomy from class may not generalize over your specific dataset: do not use!\n",
        "* Best with Word2Vec: Keyword-Based Taxonomy\n",
        "* Best with BERT: Sentence-Based Taxononomy\n",
        "\n",
        "**2. Classifies Webhose titles against developed taxonomy using: semantic_text_similarity (BERT) or word2vec similarity and groups them by topic classes.**\n",
        "\n",
        "**Final submission should include:**\n",
        "1. Jupyter (.ipynb) or Python (.py) notebook with your implementation\n",
        "2. Explicit JSON representation of your taxonomy \n",
        "3. Output showing each topic in taxonomy and top 10 matching articles\n",
        "\n",
        "(https://courseworks2.columbia.edu/courses/129128/assignments/591463?module_item_id=1211044)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIkMYjHgbgo4"
      },
      "source": [
        "#### *Load Pre-trained Model, Packages, etc.*\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Brj9BwGv5V2g"
      },
      "source": [
        "#!pip install semantic-text-similarity\n",
        "\n",
        "#load_ext google.colab.data_table\n",
        "%reload_ext google.colab.data_table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cUy34uZAUQc"
      },
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import gensim, operator\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "from scipy import spatial\n",
        "\n",
        "model_path = '/content/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqjShQhnKz5w"
      },
      "source": [
        "### *Import set of deduplicated Webhose Apple News titles (JSON)*\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_dG2V6mFLHK"
      },
      "source": [
        "# import deduplicated newsfeed dataset\n",
        "\n",
        "title = []\n",
        "\n",
        "json_data = open('/content/dedupes.json').readlines()\n",
        "\n",
        "for line in json_data:\n",
        "    title.append(json.loads(line))\n",
        "\n",
        "news_feeds = pd.DataFrame(title)\n",
        "title_df = pd.DataFrame(news_feeds['title'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-ixqWyzK6QA"
      },
      "source": [
        "### *Load in Google Word2Vec Model*\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5qjHE6zEYJ_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "a2488dd9-9d07-4482-adbf-5e782e927e35"
      },
      "source": [
        "def load_wordvec_model(modelName, modelFile, flagBin):\n",
        "    print('Loading ' + modelName + ' model...')\n",
        "    model = KeyedVectors.load_word2vec_format(model_path + modelFile, binary=flagBin)\n",
        "    print('Finished loading ' + modelName + ' model...')\n",
        "    return model\n",
        "\n",
        "model = load_wordvec_model('Word2Vec', 'GoogleNews-vectors-negative300.bin.gz', True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading Word2Vec model...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "EOFError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-e532602e7edc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_wordvec_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Word2Vec'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'GoogleNews-vectors-negative300.bin.gz'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-e532602e7edc>\u001b[0m in \u001b[0;36mload_wordvec_model\u001b[0;34m(modelName, modelFile, flagBin)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_wordvec_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodelFile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflagBin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loading '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodelName\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' model...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodelFile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflagBin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Finished loading '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodelName\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' model...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m   1436\u001b[0m         return _load_word2vec_format(\n\u001b[1;32m   1437\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1438\u001b[0;31m             limit=limit, datatype=datatype)\n\u001b[0m\u001b[1;32m   1439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1440\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_keras_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/utils_any2vec.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m    210\u001b[0m                         \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m                 \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m                 \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfromstring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbinary_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mREAL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m                 \u001b[0madd_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/gzip.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    285\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0merrno\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEBADF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"read() on write-only GzipFile object\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/_compression.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mview\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mview\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"B\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mbyte_view\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyte_view\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m             \u001b[0mbyte_view\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/gzip.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    491\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuf\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34mb\"\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m                 raise EOFError(\"Compressed file ended before the \"\n\u001b[0m\u001b[1;32m    494\u001b[0m                                \"end-of-stream marker was reached\")\n\u001b[1;32m    495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mEOFError\u001b[0m: Compressed file ended before the end-of-stream marker was reached"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXBlftIjhJFY"
      },
      "source": [
        "def vocab_check(vectors, words):\n",
        "    output = []\n",
        "    for word in words:\n",
        "        if word in vectors.vocab:\n",
        "            output.append(word.strip())\n",
        "    return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7BX1iKaF6FS"
      },
      "source": [
        "#### *Implement a keyword or sentence taxonomy of topic classes*\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szR92nu0FEyV"
      },
      "source": [
        "# custom taxonomy for apple webhose newsfeed data vocab/semantics\n",
        "\n",
        "topic_taxonomy = {\n",
        "    'News':\n",
        "\t    {\n",
        "            'Looters' : 'iPhone Looters Being Tracked - Apple Warns iPhone Looters',\n",
        "            'Hackers' : 'Apple bug exposed user accounts to hackers',\n",
        "            'Stolen' : 'Apple Warns Looters - The iPhones You Stole Are Tracked',\n",
        "            'Service Outage' : 'iCloud and other Apple services experienced an outage',\n",
        "            'Outage Impacts' : 'iCloud outage impacts Mail, web apps, more'\n",
        "        },\n",
        "\n",
        "    'iOS':\n",
        "        {\n",
        "            'iPhone' : 'iOS 14 Will Reportedly Support All iPhone Models Running iOS 13',\n",
        "            'iCloud' : 'iCloud outage took down some macOS and iOS software',\n",
        "            'Safari' :  'Safari in iOS and iPadOS 14 Might Include Built-In Translation, Better',\n",
        "            'AirDrop' : \"Sharedrop - HTML5 clone of Apple's AirDrop\",\n",
        "            'YouTube Kids' : 'Apple TV Users Can Now Enjoy YouTube Kids'\n",
        "        },\n",
        "\n",
        "    'Apple':\n",
        "        {\n",
        "            'New' : 'New iPad Air may come with USB-C not Lightning Port',\n",
        "            'Price' : 'Deutsche Bank raises Apple price target $15 to $320',\n",
        "            'Sales' : 'iPhone Sales Took a Hit Due to Coronavirus',\n",
        "            'Cloud': 'iCloud AirDrop AppleTV iTunes storage photos appleID icloud.com',\n",
        "            'CEO' : 'Tech investor urges Apple CEO Tim Cook, Chief Executive Officer'\n",
        "        },\n",
        "\n",
        "    'App Store':\n",
        "        {\n",
        "            'Apple TV':'Peloton releases dedicated Apple TV app with thousands of workouts',\n",
        "            'StopCovid': \"French govt's StopCovid tracing app\",\n",
        "            'Apple Pay': \"Why are Apple Pay, Starbucks' app, and Samsung\"\n",
        "        }\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTfeUYYaIsMQ"
      },
      "source": [
        "#### *Calculate Semantic Similarity of 2 Strings*\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfgpyuuCIr2Y"
      },
      "source": [
        "def calc_similarity(input1, input2, vectors):\n",
        "    s1words = set(vocab_check(vectors, input1.split()))\n",
        "    s2words = set(vocab_check(vectors, input2.split()))\n",
        "    output = vectors.n_similarity(s1words, s2words)\n",
        "    return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aAtJrXUGdBw"
      },
      "source": [
        "#### *Classify titles by topics using keyword max score value*\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4RcY-zfIy-o"
      },
      "source": [
        "def classify_topics(input, vectors):\n",
        "    score = []\n",
        "    feed_score = dict()\n",
        "\n",
        "    for key, value in topic_taxonomy.items():\n",
        "        max_value_score = dict()\n",
        "        for label, keywords in value.items():\n",
        "            max_value_score[label] = 0\n",
        "            topic = (key + ' ' + keywords).strip()\n",
        "            max_value_score[label] += float(calc_similarity(input, topic, vectors))\n",
        "\n",
        "        sorted_max_score = sorted(max_value_score.items(), key=operator.itemgetter(1), reverse=True)[0]\n",
        "        feed_score[sorted_max_score[0]] = sorted_max_score[1]\n",
        "        \n",
        "\n",
        "    return (sorted(feed_score.items(), key=operator.itemgetter(1), reverse=True)[:1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQy-ojRiGmi9"
      },
      "source": [
        "#### *Output each taxonomy topic and top 10 matching articles*\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTUa_fWdbUI4"
      },
      "source": [
        "f = 0\n",
        "f_max = len(title_df)\n",
        "\n",
        "tts_list = []\n",
        "score_list = []\n",
        "title_list = []\n",
        "topic_list = []\n",
        "\n",
        "for f in range(f_max):\n",
        "\n",
        "    title = str(title_df['title'][[f]])\n",
        "    title_list.append(title)\n",
        "    title_low = str(title.lower)\n",
        "\n",
        "    ts_list = classify_topics(title_low, model)\n",
        "    \n",
        "    topic = ts_list[0][0]\n",
        "    score = ts_list[0][1]\n",
        "\n",
        "    topic_list.append(topic)\n",
        "    score_list.append(score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQ1D5fHRmowg"
      },
      "source": [
        "title_df = pd.DataFrame(data = title_list, columns = ['title'])\n",
        "topic_df = pd.DataFrame(data = topic_list, columns = ['topic'])\n",
        "score_df = pd.DataFrame(data = score_list, columns = ['score'])\n",
        "\n",
        "tts_df = pd.concat([title_df, topic_df, score_df], names = ['title', 'topic', 'score'], axis = 1)\n",
        "top_df = tts_df.sort_values(['topic', 'score'], ascending = True).head(10)\n",
        "\n",
        "top_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InDNhH16Guyx"
      },
      "source": [
        "#### *Output explicit JSON representation of taxonomy*\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYnJ1JVJyK0l"
      },
      "source": [
        "with open(\"taxonomy.json\", \"w\") as data_file:\n",
        "  \n",
        "    for topic in topic_taxonomy:\n",
        "        line = json.dumps(topic)\n",
        "        data_file.write(line)\n",
        "        data_file.write(\"\\n\")\n",
        "\n",
        "json_data = open(\"taxonomy.json\").readlines()\n",
        "\n",
        "taxonomy_read = []\n",
        "\n",
        "for line in json_data:\n",
        "    taxonomy_read.append(json.loads(line))\n",
        "\n",
        "taxonomy_read"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
