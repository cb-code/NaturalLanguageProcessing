{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5430_Assignment7_chb2132.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pS2pWhqj-PLQ"
      },
      "source": [
        "# **Assignment 7**\n",
        "---\n",
        "# **5430 NLP | Spring 2021 | Uni: chb2132**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGtZBeue_cJa"
      },
      "source": [
        "## **A. Write a Python program, which:**\n",
        "### **1. Filters out exactly and/or semantically duplicate articles from Webhose dataset**\n",
        "---\n",
        "#### **Notes**\n",
        "* Use **LSH** *(SimHash: Word2Vec)* to deduplicate feeds based on title\n",
        "* Store entire feeds in **JSON, text or CSV file**\n",
        "* Include original and deduplicated sets of titles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpo7_4965-_R"
      },
      "source": [
        "### **Step 1:** Deduplicate Webhose Articles *(Exact/Semantic)*\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UyNkmYsLGSw9"
      },
      "source": [
        "!pip install simhash"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bQCBILkKeUU"
      },
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gensim, operator\n",
        "\n",
        "from gensim.models import KeyedVectors\n",
        "from scipy import spatial"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWfAKWotDcDM"
      },
      "source": [
        "#### *Load Google Word2Vec Vector Model*\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jPnwfipkB_o"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_4zCjP_MM1g"
      },
      "source": [
        "# loads in a model from specified path & file\n",
        "model_path = '/content/drive/MyDrive/'\n",
        "\n",
        "def load_model(model_name, file_name, flagBin):\n",
        "\n",
        "    print('Loading ' + model_name + ' model...')\n",
        "    model = KeyedVectors.load_word2vec_format(model_path + file_name, binary = flagBin)\n",
        "    print('Finished loading ' + model_name + ' model...')\n",
        "    \n",
        "    return model\n",
        "   \n",
        "# load in Google word2vec model\n",
        "model_w2v = load_model('Word2Vec', 'GoogleNews-vectors-negative300.bin.gz', True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YABHq_AnMLO9"
      },
      "source": [
        "#### *Load Apple Webhose Newsfeeds, Subset Titles*\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ssH72Qd7vXW"
      },
      "source": [
        "# load in Apple Webhose newsfeeds\n",
        "apple_feeds = []\n",
        "\n",
        "with open('/content/drive/MyDrive/webhose_apple.json', 'r') as f:\n",
        "  for line in f.readlines():\n",
        "    apple_feeds.append(json.loads(line))\n",
        "\n",
        "# subset newsfeed for title column\n",
        "newsfeeds = [a['title'] for a in apple_feeds]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1soJJlBH6hF"
      },
      "source": [
        "i = 0\n",
        "feeds = []\n",
        "\n",
        "for feed in apple_feeds[:10800]:\n",
        "    feed['id'] = i\n",
        "    #print(feed['id'], str(feed['title']))\n",
        "    i += 1\n",
        "    feeds.append(feed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nqOcVKxMaIA"
      },
      "source": [
        "#### *Define Vector Model Functions*\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWh1-gYWHXQS"
      },
      "source": [
        "# check if input words in model vocabulary\n",
        "def check_vocab(model, words):\n",
        "    \n",
        "    check_words = list()\n",
        "\n",
        "    for word in words:\n",
        "        if word in model.vocab:\n",
        "            check_words.append(words.strip())\n",
        "            \n",
        "    return check_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQE0rVMSHiX7"
      },
      "source": [
        "# calculate string similarity with model\n",
        "def calc_sim(s1, s2, vects):\n",
        "\n",
        "    s1_words = set(check_vocab(vects, s1.split()))\n",
        "    s2_words = set(check_vocab(vects, s2.split()))\n",
        "    \n",
        "    str_sim = vects.n_similarity(s1_words, s2_words)\n",
        "\n",
        "    return str_sim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71bn77xLIO_g"
      },
      "source": [
        "### *Define Distance and Index (SimHash)*\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9p-bVKXIcJL"
      },
      "source": [
        "import logging\n",
        "logging.getLogger('simhash').setLevel(logging.CRITICAL)\n",
        "\n",
        "from simhash import Simhash, SimhashIndex"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgrUxJ9JK_oI"
      },
      "source": [
        "# define distance\n",
        "hamming_dist = 2\n",
        "\n",
        "# define SimHash index and determine duplicate titles\n",
        "obj = [(str(feed['id']), Simhash(str(feed['title']))) for feed in feeds]\n",
        "\n",
        "index = SimhashIndex(obj, k = hamming_dist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phl6XbVo5gLY"
      },
      "source": [
        "### *Calculate and Print Duplicate Titles (SimHash)*---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfVAks5PIiIH"
      },
      "source": [
        "f = 0\n",
        "dd_dex = []\n",
        "dupe_ct = []\n",
        "dedupe_dex = []\n",
        "\n",
        "for feed in feeds:\n",
        "\n",
        "  feed_sel = feeds[f]\n",
        "    \n",
        "  # calculate  hash value\n",
        "  feed_hash = Simhash(str(feed_sel['title']))\n",
        "    \n",
        "  # find all duplicate indices\n",
        "  dupe_dex = index.get_near_dups(feed_hash)\n",
        "  dupe_ct.append(dupe_dex)\n",
        "  \n",
        "  if(len(dupe_dex) > 1):\n",
        "    dedupe_dex.append(dupe_dex)\n",
        "  \n",
        "  f += 1\n",
        "\n",
        "d = 0\n",
        "\n",
        "for dupe in dedupe_dex:\n",
        "\n",
        "  dex = dedupe_dex[d]\n",
        "\n",
        "  for e in dex:\n",
        "    dd_dex.append(int(e))\n",
        "\n",
        "  d += 1\n",
        "\n",
        "ddp = sorted(np.unique(dd_dex))\n",
        "#print(ddp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PthCatL8z8Rt"
      },
      "source": [
        "dedupes = []\n",
        "j = 0\n",
        "\n",
        "for feed in feeds:\n",
        "  if feed['id'] not in ddp:\n",
        "    dedupes.append(feed)\n",
        "    j += 1\n",
        "\n",
        "#print(len(dedupes))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Lsyr5pyIshA"
      },
      "source": [
        "### *Calculate SimHash/Word2Vec Similarity*\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cf9sGJZxJNyZ"
      },
      "source": [
        "# calculate similarity with SimHash/Word2Vec\n",
        "dedupe = []\n",
        "dupe_count = 0\n",
        "\n",
        "for dupe in dupe_dex:\n",
        "  \n",
        "  try:\n",
        "    dupe_score = sim_calc(feed_sel['title'], feeds[int(dupe)]['title'], model_w2v)\n",
        "    \n",
        "  except:\n",
        "    dupe_score = 0\n",
        "  \n",
        "  if dupe_score > 0.7:\n",
        "    dupe_count += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxnoQWB4_AYK"
      },
      "source": [
        "### *Write deduplicated articles to a new JSON file*\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vjd9S9_7-_zI"
      },
      "source": [
        "with open(\"dedupes.json\", \"w\") as data_file:\n",
        "  \n",
        "  for dupe in dedupes:\n",
        "\n",
        "    line = json.dumps(dupe)\n",
        "    data_file.write(line)\n",
        "    data_file.write(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4YvbU8S_W5G"
      },
      "source": [
        "### *Read and print record count to output*\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GoQkYrTlAs73"
      },
      "source": [
        "# load in deduplicated json file and read to list\n",
        "dedupeson = []\n",
        "\n",
        "with open('/content/drive/MyDrive/dedupes.json', 'r') as f:\n",
        "  for dedupe in f.readlines():\n",
        "    dedupeson.append(json.loads(dedupe))\n",
        "\n",
        "# print record count from dedupe json to output\n",
        "print(\"Deduped Count: \" + str(len(dedupeson[0])))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
